{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natasha-kolom/Machine_learning/blob/main/%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B61.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення вхідних даних і цілей у тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a98a195-7129-43fb-f72c-46526bef9d47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb643072-4d14-45cd-f1f5-2845946010d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7df1f7e1cad0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee73f77-8e79-48ee-c5fb-f17a4ae6eb7f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0276, -0.5631, -0.8923]], requires_grad=True)\n",
            "tensor([-0.0583], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо модель\n",
        "def model(x, w, b):\n",
        "    return 1/(1+torch.exp(-(x @ w.t() + b)))"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генеруємо передбачення\n",
        "preds = model(inputs, w, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkZeInEtmQZp",
        "outputId": "afdca5cc-8453-4008-ec13-92b639b15a14"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** пробувала декілька разів рандомно ініціювати ваги і передбачення виходять або всі 1 або нулі, це може бути пов'язано з тим, що неправильні ваги ініціюють логарифм нуля і всі передбачення або близькі до 0, або до одиниці"
      ],
      "metadata": {
        "id": "U4t97dCO0WkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "def binary_cross_entropy(true_labels, predicted_probs):\n",
        "    epsilon = 1e-6  # Для уникнення log(0)\n",
        "    predicted_probs = torch.clamp(predicted_probs, epsilon, 1. - epsilon)\n",
        "    loss = -(true_labels*torch.log(predicted_probs)+(1-true_labels)*torch.log(1-predicted_probs))\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = binary_cross_entropy(targets, preds)\n",
        "print(loss1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcOSMW9JowmB",
        "outputId": "b69295d0-cfb5-4a00-b61f-187e5ff378ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.5209, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss1.backward(retain_graph=True)\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26b3985-1f81-4647-fd40-a9e53d9d97ac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.]])\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** Думаю, що градієнти нульові (тобто функція не змінюється) тому що випадкові ваги дають нам результат занадто близько до нуля або до 1"
      ],
      "metadata": {
        "id": "k1-dH21u1EE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3739b4-5b3d-4b33-9c3d-9e2ceee014ab"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(targets, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBNIrQa3KBrM",
        "outputId": "93b5e4d8-6574-425c-d3bf-8846c8a2c310"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6829, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward(retain_graph=True)\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4RAe8ybKL7z",
        "outputId": "264db265-6dd3-4af1-a0cb-470e868fe825"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислюємо loss\n",
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7GVynR0LoqU",
        "outputId": "a1eb8144-8cf7-45fa-92d5-82d1ea59e9aa"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2282, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeIeN-QxLvs0",
        "outputId": "f78e4756-b01c-4794-9af3-58e498d9c49e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5577],\n",
              "        [0.7774],\n",
              "        [0.9921],\n",
              "        [0.0071],\n",
              "        [0.9887]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdL8gqV2Lwq8",
        "outputId": "8ee2af97-1bca-4faf-e9d3-b48151186028"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** Якщо брати поріг 0.5, то прогнози за цією моделлю достатньо точні"
      ],
      "metadata": {
        "id": "vPnUhi341Y2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Імпортуємо tensor dataset & data loader\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "p5vBuLlSuXoT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "# Визначаємо dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf369c0-baa5-4e9f-8c68-1247314f6cf6"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "first_batch = next(iter(train_dl))\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217540fb-17fe-4ce1-abed-3dc58386f75a"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 69.,  96.,  70.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 73.,  67.,  43.],\n",
            "        [ 73.,  67.,  43.]]), tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "XAK_dNJHwBjU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg (nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "        self.act = nn.Sigmoid() # Activation function\n",
        "\n",
        "\n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogReg()"
      ],
      "metadata": {
        "id": "BeL11YjYxaxk"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss = torch.nn.functional.binary_cross_entropy"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(inputs)\n",
        "\n",
        "# Обчислення втрат\n",
        "loss_final = loss(predictions, targets)\n",
        "print(loss_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VIekwmsx5tl",
        "outputId": "79de2d82-f422-4fb7-a414-03658a1434c1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4403, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.round(predictions * 100) / 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6s7G1ZXy1x8",
        "outputId": "3471b4e2-b3f6-4e00-aa2c-b5d1d870ef6d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [0.2300],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [0.2300],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [0.2300],\n",
              "        [1.0000]], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yYKy5V0y3lS",
        "outputId": "8f478858-0b93-4cec-e359-f9980adc169f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** Модель достатньо добре навчилась передбачати результати"
      ],
      "metadata": {
        "id": "C7NsQ1gN1rOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Модифікована функцію fit для відстеження втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = fit_return_loss(1000, model, loss, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PldANZ4zScm",
        "outputId": "9c233f9b-038e-413c-e119-01defb6b4998"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 26.4281\n",
            "Epoch [20/1000], Loss: 25.8322\n",
            "Epoch [30/1000], Loss: 25.3428\n",
            "Epoch [40/1000], Loss: 24.8545\n",
            "Epoch [50/1000], Loss: 24.4238\n",
            "Epoch [60/1000], Loss: 24.1126\n",
            "Epoch [70/1000], Loss: 23.9218\n",
            "Epoch [80/1000], Loss: 23.8006\n",
            "Epoch [90/1000], Loss: 23.7129\n",
            "Epoch [100/1000], Loss: 23.6455\n",
            "Epoch [110/1000], Loss: 23.5700\n",
            "Epoch [120/1000], Loss: 23.5126\n",
            "Epoch [130/1000], Loss: 23.4451\n",
            "Epoch [140/1000], Loss: 23.3940\n",
            "Epoch [150/1000], Loss: 23.3243\n",
            "Epoch [160/1000], Loss: 23.2781\n",
            "Epoch [170/1000], Loss: 23.2153\n",
            "Epoch [180/1000], Loss: 23.1709\n",
            "Epoch [190/1000], Loss: 23.1197\n",
            "Epoch [200/1000], Loss: 23.0860\n",
            "Epoch [210/1000], Loss: 23.0563\n",
            "Epoch [220/1000], Loss: 23.0423\n",
            "Epoch [230/1000], Loss: 23.0338\n",
            "Epoch [240/1000], Loss: 23.0107\n",
            "Epoch [250/1000], Loss: 23.0034\n",
            "Epoch [260/1000], Loss: 22.9916\n",
            "Epoch [270/1000], Loss: 22.9998\n",
            "Epoch [280/1000], Loss: 22.9954\n",
            "Epoch [290/1000], Loss: 22.9718\n",
            "Epoch [300/1000], Loss: 22.9729\n",
            "Epoch [310/1000], Loss: 22.9638\n",
            "Epoch [320/1000], Loss: 22.9460\n",
            "Epoch [330/1000], Loss: 22.9372\n",
            "Epoch [340/1000], Loss: 22.9387\n",
            "Epoch [350/1000], Loss: 22.9187\n",
            "Epoch [360/1000], Loss: 22.9172\n",
            "Epoch [370/1000], Loss: 22.9051\n",
            "Epoch [380/1000], Loss: 22.9058\n",
            "Epoch [390/1000], Loss: 22.8949\n",
            "Epoch [400/1000], Loss: 22.8882\n",
            "Epoch [410/1000], Loss: 22.8712\n",
            "Epoch [420/1000], Loss: 22.8664\n",
            "Epoch [430/1000], Loss: 22.8557\n",
            "Epoch [440/1000], Loss: 22.8483\n",
            "Epoch [450/1000], Loss: 22.8456\n",
            "Epoch [460/1000], Loss: 22.8371\n",
            "Epoch [470/1000], Loss: 22.8323\n",
            "Epoch [480/1000], Loss: 22.8175\n",
            "Epoch [490/1000], Loss: 22.8105\n",
            "Epoch [500/1000], Loss: 22.8055\n",
            "Epoch [510/1000], Loss: 22.7967\n",
            "Epoch [520/1000], Loss: 22.7947\n",
            "Epoch [530/1000], Loss: 22.7791\n",
            "Epoch [540/1000], Loss: 22.7737\n",
            "Epoch [550/1000], Loss: 22.7670\n",
            "Epoch [560/1000], Loss: 22.7589\n",
            "Epoch [570/1000], Loss: 22.7479\n",
            "Epoch [580/1000], Loss: 22.7361\n",
            "Epoch [590/1000], Loss: 22.7305\n",
            "Epoch [600/1000], Loss: 22.7225\n",
            "Epoch [610/1000], Loss: 22.7170\n",
            "Epoch [620/1000], Loss: 22.7144\n",
            "Epoch [630/1000], Loss: 22.7007\n",
            "Epoch [640/1000], Loss: 22.6873\n",
            "Epoch [650/1000], Loss: 22.6859\n",
            "Epoch [660/1000], Loss: 22.6772\n",
            "Epoch [670/1000], Loss: 22.6723\n",
            "Epoch [680/1000], Loss: 22.6578\n",
            "Epoch [690/1000], Loss: 22.6509\n",
            "Epoch [700/1000], Loss: 22.6424\n",
            "Epoch [710/1000], Loss: 22.6328\n",
            "Epoch [720/1000], Loss: 22.6217\n",
            "Epoch [730/1000], Loss: 22.6162\n",
            "Epoch [740/1000], Loss: 22.6146\n",
            "Epoch [750/1000], Loss: 22.6046\n",
            "Epoch [760/1000], Loss: 22.6027\n",
            "Epoch [770/1000], Loss: 22.5858\n",
            "Epoch [780/1000], Loss: 22.5791\n",
            "Epoch [790/1000], Loss: 22.5706\n",
            "Epoch [800/1000], Loss: 22.5732\n",
            "Epoch [810/1000], Loss: 22.5563\n",
            "Epoch [820/1000], Loss: 22.5449\n",
            "Epoch [830/1000], Loss: 22.5395\n",
            "Epoch [840/1000], Loss: 22.5316\n",
            "Epoch [850/1000], Loss: 22.5231\n",
            "Epoch [860/1000], Loss: 22.5126\n",
            "Epoch [870/1000], Loss: 22.5040\n",
            "Epoch [880/1000], Loss: 22.4997\n",
            "Epoch [890/1000], Loss: 22.4903\n",
            "Epoch [900/1000], Loss: 22.4827\n",
            "Epoch [910/1000], Loss: 22.4746\n",
            "Epoch [920/1000], Loss: 22.4673\n",
            "Epoch [930/1000], Loss: 22.4558\n",
            "Epoch [940/1000], Loss: 22.4566\n",
            "Epoch [950/1000], Loss: 22.4400\n",
            "Epoch [960/1000], Loss: 22.4314\n",
            "Epoch [970/1000], Loss: 22.4236\n",
            "Epoch [980/1000], Loss: 22.4152\n",
            "Epoch [990/1000], Loss: 22.4105\n",
            "Epoch [1000/1000], Loss: 22.3997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "4nMb5O9dzm_t",
        "outputId": "e240c548-a289-4be8-a606-ea41c62d5e48"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds.round()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUr6GDHR1-LY",
        "outputId": "3f3dfc77-abfa-4d19-95db-753f8bc073a7"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.]], grad_fn=<RoundBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RqwTBjK50Ft",
        "outputId": "2b11ac57-194e-4c7e-e8c0-82fdf461dafb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXiGR9p9570q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}