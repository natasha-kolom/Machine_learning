{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natasha-kolom/Machine_learning/blob/main/%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Перетворення вхідних даних і цілей у тензори\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a98a195-7129-43fb-f72c-46526bef9d47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb643072-4d14-45cd-f1f5-2845946010d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7df1f7e1cad0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee73f77-8e79-48ee-c5fb-f17a4ae6eb7f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0276, -0.5631, -0.8923]], requires_grad=True)\n",
            "tensor([-0.0583], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо модель\n",
        "def model(x, w, b):\n",
        "    return 1/(1+torch.exp(-(x @ w.t() + b)))"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генеруємо передбачення\n",
        "preds = model(inputs, w, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkZeInEtmQZp",
        "outputId": "afdca5cc-8453-4008-ec13-92b639b15a14"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** пробувала декілька разів рандомно ініціювати ваги і передбачення виходять або всі 1 або нулі, це може бути пов'язано з тим, що неправильні ваги ініціюють логарифм нуля і всі передбачення або близькі до 0, або до одиниці"
      ],
      "metadata": {
        "id": "U4t97dCO0WkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "def binary_cross_entropy(true_labels, predicted_probs):\n",
        "    epsilon = 1e-6  # Для уникнення log(0)\n",
        "    predicted_probs = torch.clamp(predicted_probs, epsilon, 1. - epsilon)\n",
        "    loss = -(true_labels*torch.log(predicted_probs)+(1-true_labels)*torch.log(1-predicted_probs))\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = binary_cross_entropy(targets, preds)\n",
        "print(loss1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcOSMW9JowmB",
        "outputId": "b69295d0-cfb5-4a00-b61f-187e5ff378ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.5209, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss1.backward(retain_graph=True)\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26b3985-1f81-4647-fd40-a9e53d9d97ac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.]])\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** Думаю, що градієнти нульові (тобто функція не змінюється) тому що випадкові ваги дають нам результат занадто близько до нуля або до 1"
      ],
      "metadata": {
        "id": "k1-dH21u1EE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3739b4-5b3d-4b33-9c3d-9e2ceee014ab"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(targets, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBNIrQa3KBrM",
        "outputId": "93b5e4d8-6574-425c-d3bf-8846c8a2c310"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6829, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward(retain_graph=True)\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4RAe8ybKL7z",
        "outputId": "264db265-6dd3-4af1-a0cb-470e868fe825"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обчислюємо loss\n",
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7GVynR0LoqU",
        "outputId": "a1eb8144-8cf7-45fa-92d5-82d1ea59e9aa"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2282, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeIeN-QxLvs0",
        "outputId": "f78e4756-b01c-4794-9af3-58e498d9c49e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5577],\n",
              "        [0.7774],\n",
              "        [0.9921],\n",
              "        [0.0071],\n",
              "        [0.9887]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdL8gqV2Lwq8",
        "outputId": "8ee2af97-1bca-4faf-e9d3-b48151186028"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** Якщо брати поріг 0.5, то прогнози за цією моделлю достатньо точні"
      ],
      "metadata": {
        "id": "vPnUhi341Y2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Імпортуємо tensor dataset & data loader\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "p5vBuLlSuXoT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "# Визначаємо dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2275575e-ed63-4348-d3a5-caebc6961488"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначаємо data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "first_batch = next(iter(train_dl))\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2920ef0-f48d-4fca-a89d-a60926b5e268"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 87., 134.,  58.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [102.,  43.,  37.],\n",
            "        [102.,  43.,  37.]]), tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "XAK_dNJHwBjU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg (nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "        self.act = nn.Sigmoid() # Activation function\n",
        "\n",
        "\n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogReg()"
      ],
      "metadata": {
        "id": "BeL11YjYxaxk"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss = torch.nn.functional.binary_cross_entropy"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(inputs)\n",
        "\n",
        "# Обчислення втрат\n",
        "loss_final = loss(predictions, targets)\n",
        "print(loss_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VIekwmsx5tl",
        "outputId": "79de2d82-f422-4fb7-a414-03658a1434c1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.4403, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.round(predictions * 100) / 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6s7G1ZXy1x8",
        "outputId": "3471b4e2-b3f6-4e00-aa2c-b5d1d870ef6d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [0.2300],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [0.2300],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [1.0000],\n",
              "        [0.2300],\n",
              "        [1.0000]], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yYKy5V0y3lS",
        "outputId": "8f478858-0b93-4cec-e359-f9980adc169f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Спостереження** Модель достатньо добре навчилась передбачати результати"
      ],
      "metadata": {
        "id": "C7NsQ1gN1rOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Модифікована функцію fit для відстеження втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses, pred"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = fit_return_loss(1000, model, loss, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PldANZ4zScm",
        "outputId": "6b6e73b1-8356-4ac2-943f-d26c7b5837f1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 8.2426\n",
            "Epoch [20/1000], Loss: 5.8574\n",
            "Epoch [30/1000], Loss: 3.5053\n",
            "Epoch [40/1000], Loss: 1.8991\n",
            "Epoch [50/1000], Loss: 1.3070\n",
            "Epoch [60/1000], Loss: 0.9844\n",
            "Epoch [70/1000], Loss: 0.7283\n",
            "Epoch [80/1000], Loss: 0.5167\n",
            "Epoch [90/1000], Loss: 0.3516\n",
            "Epoch [100/1000], Loss: 0.2540\n",
            "Epoch [110/1000], Loss: 0.2049\n",
            "Epoch [120/1000], Loss: 0.1840\n",
            "Epoch [130/1000], Loss: 0.1768\n",
            "Epoch [140/1000], Loss: 0.1728\n",
            "Epoch [150/1000], Loss: 0.1705\n",
            "Epoch [160/1000], Loss: 0.1699\n",
            "Epoch [170/1000], Loss: 0.1700\n",
            "Epoch [180/1000], Loss: 0.1693\n",
            "Epoch [190/1000], Loss: 0.1696\n",
            "Epoch [200/1000], Loss: 0.1691\n",
            "Epoch [210/1000], Loss: 0.1695\n",
            "Epoch [220/1000], Loss: 0.1689\n",
            "Epoch [230/1000], Loss: 0.1688\n",
            "Epoch [240/1000], Loss: 0.1692\n",
            "Epoch [250/1000], Loss: 0.1693\n",
            "Epoch [260/1000], Loss: 0.1690\n",
            "Epoch [270/1000], Loss: 0.1684\n",
            "Epoch [280/1000], Loss: 0.1681\n",
            "Epoch [290/1000], Loss: 0.1683\n",
            "Epoch [300/1000], Loss: 0.1687\n",
            "Epoch [310/1000], Loss: 0.1681\n",
            "Epoch [320/1000], Loss: 0.1680\n",
            "Epoch [330/1000], Loss: 0.1677\n",
            "Epoch [340/1000], Loss: 0.1679\n",
            "Epoch [350/1000], Loss: 0.1678\n",
            "Epoch [360/1000], Loss: 0.1677\n",
            "Epoch [370/1000], Loss: 0.1676\n",
            "Epoch [380/1000], Loss: 0.1675\n",
            "Epoch [390/1000], Loss: 0.1688\n",
            "Epoch [400/1000], Loss: 0.1673\n",
            "Epoch [410/1000], Loss: 0.1677\n",
            "Epoch [420/1000], Loss: 0.1669\n",
            "Epoch [430/1000], Loss: 0.1676\n",
            "Epoch [440/1000], Loss: 0.1677\n",
            "Epoch [450/1000], Loss: 0.1669\n",
            "Epoch [460/1000], Loss: 0.1668\n",
            "Epoch [470/1000], Loss: 0.1675\n",
            "Epoch [480/1000], Loss: 0.1667\n",
            "Epoch [490/1000], Loss: 0.1666\n",
            "Epoch [500/1000], Loss: 0.1672\n",
            "Epoch [510/1000], Loss: 0.1671\n",
            "Epoch [520/1000], Loss: 0.1661\n",
            "Epoch [530/1000], Loss: 0.1663\n",
            "Epoch [540/1000], Loss: 0.1659\n",
            "Epoch [550/1000], Loss: 0.1661\n",
            "Epoch [560/1000], Loss: 0.1660\n",
            "Epoch [570/1000], Loss: 0.1673\n",
            "Epoch [580/1000], Loss: 0.1658\n",
            "Epoch [590/1000], Loss: 0.1662\n",
            "Epoch [600/1000], Loss: 0.1661\n",
            "Epoch [610/1000], Loss: 0.1656\n",
            "Epoch [620/1000], Loss: 0.1662\n",
            "Epoch [630/1000], Loss: 0.1654\n",
            "Epoch [640/1000], Loss: 0.1653\n",
            "Epoch [650/1000], Loss: 0.1653\n",
            "Epoch [660/1000], Loss: 0.1659\n",
            "Epoch [670/1000], Loss: 0.1655\n",
            "Epoch [680/1000], Loss: 0.1650\n",
            "Epoch [690/1000], Loss: 0.1649\n",
            "Epoch [700/1000], Loss: 0.1653\n",
            "Epoch [710/1000], Loss: 0.1666\n",
            "Epoch [720/1000], Loss: 0.1647\n",
            "Epoch [730/1000], Loss: 0.1653\n",
            "Epoch [740/1000], Loss: 0.1650\n",
            "Epoch [750/1000], Loss: 0.1644\n",
            "Epoch [760/1000], Loss: 0.1657\n",
            "Epoch [770/1000], Loss: 0.1656\n",
            "Epoch [780/1000], Loss: 0.1642\n",
            "Epoch [790/1000], Loss: 0.1648\n",
            "Epoch [800/1000], Loss: 0.1640\n",
            "Epoch [810/1000], Loss: 0.1639\n",
            "Epoch [820/1000], Loss: 0.1645\n",
            "Epoch [830/1000], Loss: 0.1651\n",
            "Epoch [840/1000], Loss: 0.1637\n",
            "Epoch [850/1000], Loss: 0.1636\n",
            "Epoch [860/1000], Loss: 0.1640\n",
            "Epoch [870/1000], Loss: 0.1635\n",
            "Epoch [880/1000], Loss: 0.1641\n",
            "Epoch [890/1000], Loss: 0.1638\n",
            "Epoch [900/1000], Loss: 0.1646\n",
            "Epoch [910/1000], Loss: 0.1631\n",
            "Epoch [920/1000], Loss: 0.1635\n",
            "Epoch [930/1000], Loss: 0.1630\n",
            "Epoch [940/1000], Loss: 0.1629\n",
            "Epoch [950/1000], Loss: 0.1628\n",
            "Epoch [960/1000], Loss: 0.1634\n",
            "Epoch [970/1000], Loss: 0.1633\n",
            "Epoch [980/1000], Loss: 0.1630\n",
            "Epoch [990/1000], Loss: 0.1623\n",
            "Epoch [1000/1000], Loss: 0.1624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss1[0])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "4nMb5O9dzm_t",
        "outputId": "36eb69c4-1077-440e-a485-9b879abb230f"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.round(loss1[1] * 100) / 100\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUr6GDHR1-LY",
        "outputId": "f6552d27-2370-4eda-9b68-e8e6f04f172b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3800],\n",
              "        [1.0000],\n",
              "        [0.0000],\n",
              "        [1.0000],\n",
              "        [0.7100]], grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RqwTBjK50Ft",
        "outputId": "2b11ac57-194e-4c7e-e8c0-82fdf461dafb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXiGR9p9570q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}